{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 - Advanced Regression Techniques\n",
    "In this notebook we will investigate some popular advanced regression techniques:  \n",
    "* XGBoost\n",
    "* Random Forest\n",
    "* MultiLayer Perceptron (Neural Network)\n",
    "  \n",
    "We will use the exact same dataset and features as before and compare the results with our Linear Regressor.  \n",
    "You will be happy to learn that the same procedure for training a Linear Regressor applies to nearly all other regression models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load the data and remind ourselves of the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/sf/data_clean_engineered.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in df.columns if feature != 'price']\n",
    "X = df[features]\n",
    "y = df['price']\n",
    "X_np = X.values\n",
    "y_np = y.values.reshape((len(df), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=123) # split 70% train, 30% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X) # predict y values from input X\n",
    "    mse = mean_squared_error(y_true=y, y_pred=y_pred)\n",
    "    print(\"Mean Squared Error: {}\".format(mse))\n",
    "    print(\"Accuracy: {}%\".format(model.score(X, y)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - Extreme Gradient Boosting\n",
    "XGBoost is a popular machine learning algorithm applied to tabulated data. If tuned properly, it can perform very well across many different datasets and we can even visualize the \"feature importance\" and get an idea of how the model generates its prediction.  \n",
    "  \n",
    "High accuracy *AND* intuitive results? Sign me up!  \n",
    "  \n",
    "  \n",
    "Before proceeding, let's watch a few quick clips to learn more about **Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(id='2Mg8QD0F1dQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(id='GM3CDQfQ4sw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If time permits: Gradient Boosting whiteboard example\n",
    "http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the xgboost library and fit our regressor same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_regressor = XGBRegressor()\n",
    "xgb_model = xgb_regressor.fit(X_train, y_train)\n",
    "evaluate_model(xgb_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Feature Importance that XGBRegressor has assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe of feature importances\n",
    "feature_importances = pd.DataFrame(columns=X.columns)\n",
    "feature_importances.loc[0] = xgb_model.feature_importances_\n",
    "# melt columns so we can easily sort and visualize\n",
    "df_melt = pd.melt(feature_importances, value_vars=X.columns).sort_values(by='value', ascending=False)\n",
    "df_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain on entire dataset and save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb_regressor.fit(X, y)\n",
    "with open('./models/sf/xgb.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a few different regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest  \n",
    "https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor()\n",
    "rf_model = rf_regressor.fit(X_train, y_train)\n",
    "evaluate_model(rf_model, X_val, y_val)\n",
    "rf_model = rf_regressor.fit(X, y)\n",
    "with open('./models/sf/random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Perceptron\n",
    "https://en.wikipedia.org/wiki/Multilayer_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_regressor = MLPRegressor(max_iter=20000, random_state=123, solver='lbfgs')\n",
    "mlp_model = mlp_regressor.fit(X_train, y_train)\n",
    "evaluate_model(mlp_model, X_val, y_val)\n",
    "mlp_model = mlp_regressor.fit(X, y)\n",
    "with open('./models/sf/mlp.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "krr_regressor = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5, )\n",
    "krr_model = krr_regressor.fit(X_train, y_train)\n",
    "evaluate_model(krr_model, X_val, y_val)\n",
    "krr_model = krr_regressor.fit(X, y)\n",
    "with open('./models/sf/krr.pkl', 'wb') as f:\n",
    "    pickle.dump(krr_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "eln_regressor = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3, max_iter=1000)\n",
    "eln_model = eln_regressor.fit(X_train, y_train)\n",
    "evaluate_model(eln_model, X_val, y_val)\n",
    "eln_model = eln_regressor.fit(X, y)\n",
    "with open('./models/sf/eln.pkl', 'wb') as f:\n",
    "    pickle.dump(eln_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
