{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with an Exploratory Data Analysis (EDA) of our SF housing dataset.  \n",
    "It is always a good idea to start with an EDA before designing and training a machine learning algorithm.  \n",
    "EDA gives us better insight to the data by using statistical and visualization techniques.  \n",
    "\n",
    "Upon completing this notebook, we should have:  \n",
    "* Familiarity with [Pandas] and [NumPy] for data management and analysis\n",
    "* Familiarity with [Matplotlib] and [seaborn] for visualization\n",
    "* A decent understanding of the characteristics of our dataset\n",
    "[Pandas]: https://pandas.pydata.org/\n",
    "[NumPy]: http://www.numpy.org/\n",
    "[Matplotlib]: https://matplotlib.org/\n",
    "[seaborn]: https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy import Nominatim\n",
    "import geojson\n",
    "import folium\n",
    "from branca.colormap import LinearColormap, StepColormap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by loading the data and have a peek at the contents \n",
    "The data was scraped from [zillow.com](https://www.zillow.com/) and is dispersed between several csv files.  \n",
    "We will use Pandas to load the csv files and concatenate them into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csvs = []\n",
    "# load the csv files from all scraping runs\n",
    "for filename in glob.glob('./data/sf/**/*.csv'):\n",
    "    all_csvs.append(pd.read_csv(filename))\n",
    "# combine all dataframes together and drop any duplicate entries\n",
    "df_raw = pd.concat(all_csvs, ignore_index=True).drop_duplicates()\n",
    "print(f\"Found a total of {len(df_raw)} data points\")\n",
    "# save this combined dataframe as csv for safe keeping\n",
    "df_raw.to_csv('./data/sf/data_raw.csv', index=False)\n",
    "# display first 5 entries of DataFrame\n",
    "df_raw.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data is now contained in a variable named `df_raw` which is a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: Let's start the `generate_latlng()` now so we don't have to wait "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some quick stats about the DataFrame\n",
    "DataFrame has a few built in functions we can call to get a quick summary of the data:  \n",
    "* `info()` displays a count of all non-null objects and their datatypes  \n",
    "* `describe()` calculates basic statistics about all numerical values in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `describe()` method does not yet have any numerical data to describe for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a lot of the columns which provide numerical data are not in a format ready for consumption.  \n",
    "For instance the `price` columns contain `$` and `,` characters and the `facts and features` column has valuable information about number of beds, bath and square footage embedded within the text.  \n",
    "  \n",
    "Let's parse and format these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy our original dataframe for safe keeping. We will manipulate `df` instead\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat price column\n",
    "Time to use some convenient Pandas functions such as `.apply()` to apply a user defined formatting function to all values in a column.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `$` and `,` characters and format as `int`.  \n",
    "Also, some prices are represented as `$1K` and `$1M` so let's replace with `1000` and `1000000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def format_price(price):\n",
    "    \"\"\"Remove all non-numerical\"\"\"\n",
    "    price = str(price)\n",
    "    multiply_factor = 1\n",
    "    if 'M' in price:\n",
    "        multiply_factor = 1e6\n",
    "    elif 'K' in price:\n",
    "        multiply_factor = 1e3\n",
    "    non_decimal = re.compile(r'[^0-9\\.]')\n",
    "    price_num = None\n",
    "    try:\n",
    "        price_num = float(non_decimal.sub('', price))*multiply_factor\n",
    "    except Exception as e:\n",
    "#         print(f'error converting \\\"{price}\\\": {e}')\n",
    "        pass\n",
    "    finally:\n",
    "        return price_num\n",
    "\n",
    "# replace the values in the price column with the formatted price\n",
    "df['price'] = df.price.apply(format_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse `facts and features` column into multiple columns \n",
    "An example entry in this column: `3 bds , 2 ba , 1,520 sqft`  \n",
    "Parse the text using comma followed by a space '`, `' as the delimiter so that we can still capture the comma in the square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider studio as 0 beds?\n",
    "non_decimal = re.compile(r'[^\\d.]+')\n",
    "def parse_beds(string):\n",
    "    strings = string.lower().split(', ')\n",
    "    num_beds = None\n",
    "    for s in strings:\n",
    "        if \"bd\" in s:\n",
    "            try:\n",
    "                num_beds = float(non_decimal.sub('', s))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        # treat studio as 0 bedrooms\n",
    "        elif \"studio\" in s.lower():\n",
    "            num_beds = 0\n",
    "        return num_beds\n",
    "\n",
    "def parse_bath(string):\n",
    "    strings = string.lower().split(', ')\n",
    "    num_bath = None\n",
    "    for s in strings:\n",
    "        if \"ba\" in s:\n",
    "            try:\n",
    "                num_bath = float(non_decimal.sub('', s))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                return num_bath\n",
    "def parse_sqft(string):\n",
    "    strings = string.lower().split(', ')\n",
    "    sqft = None\n",
    "    for s in strings:\n",
    "        if \"ft\" in s:\n",
    "            try:\n",
    "                sqft = float(non_decimal.sub('', s))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                return sqft\n",
    "df['bed'] = df['facts and features'].apply(parse_beds)\n",
    "df['bath'] = df['facts and features'].apply(parse_bath)\n",
    "df['sqft'] = df['facts and features'].apply(parse_sqft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse `title` column into `property_type`\n",
    "The title of the posting contains some information we can parse. For instance we can map `'Condo For Sale'` --> `condo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see if there is a pattern to the titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.title.unique())\n",
    "print(df.title.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a limited amount of unique values, which is good. We can design our parser to catch most cases.  \n",
    "We won't parse 'For Sale by Owner' since it is too vague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map property types\n",
    "property_types = {'Condo For Sale': 'condo', \n",
    "                  'House For Sale': 'house', \n",
    "                  'Apartment For Sale': 'apartment', \n",
    "                  'New Construction': 'new',\n",
    "                  'Foreclosure': 'foreclosure', \n",
    "                   'Lot/Land For Sale': 'lot', \n",
    "                  'Coming Soon': 'coming', \n",
    "                  'Co-op For Sale': 'coop',\n",
    "                  'Auction': 'auction', \n",
    "                  'For Sale by Owner': None, \n",
    "                  'Townhouse For Sale': 'townhouse'}\n",
    "def parse_property_type(string):\n",
    "    try:\n",
    "        property_type = property_types[string]\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        property_type = None\n",
    "    finally:\n",
    "        return property_type\n",
    "df['property_type'] = df['title'].apply(parse_property_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.property_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the `info()` of the dataframe, we should see some columns are now numerical (`float64`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `describe()` to get some stats about the numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very large maximum price albeit not suprising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe only the 'price' column\n",
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have the gist of the dataset size and its contents, it's time to go more in depth and Visualize the data.  \n",
    "We will use `Seaborn` to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histogram of prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globally set our seaborn plot size to 12 by 8 inches:\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "def plot_prices(df: pd.DataFrame, bins: list):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(bins)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    return sns.distplot(df.price, bins=bins)\n",
    "\n",
    "bins = range(int(df.price.min()), int(df.price.max()), 500000)\n",
    "plot_prices(df.dropna(), bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitely a skewed distribution, looks as if we have a few outliers at the higher range of the prices.  \n",
    "### We can quantify how \"non-normal\" our distribution is by calculating:  \n",
    "* `Skewness` - A measure of the symmetry (or lack thereof) of a distribution\n",
    "* `Kurtosis` - Whether distrubition is \"heavy-tailed\" or \"light-tailed\" or in other words: how \"sharp\" the peak is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['price'].skew())\n",
    "print(\"Kurtosis: %f\" % df['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot with outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df[df.price < 8e6]\n",
    "bins = range(int(df_no_outliers.price.min()),int(df_no_outliers.price.max()),500000)\n",
    "plot_prices(df_no_outliers, bins)\n",
    "print(\"Skewness (outliers removed): %f\" % df_no_outliers['price'].skew())\n",
    "print(\"Kurtosis (outliers removed): %f\" % df_no_outliers['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the outliers improved our skewness and kurtosis values.\n",
    "We will remember this when cleaning the data for our model. Machine learning models work best with normally distributed data. Outliers may affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot missing values.\n",
    "Recall that there were some columns which are incomplete. Plot a bar graph describing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that are missing values can either be removed from the dataset or have their missing values replaced (perhaps with 0 or the mean of the column). Remember this for data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the house prices w.r.t. location with a slippy map  \n",
    "We have some location information in the `address` column. We'll use geocoding to convert the string address to Lat Long.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from geopy.geocoders import Nominatim\n",
    "NUM_RETRIES = 3 # number of retries for request\n",
    "def generate_latlng(dataframe: pd.DataFrame):\n",
    "    dataframe = dataframe.copy()\n",
    "    geocoder = Nominatim()\n",
    "    latlngs = []\n",
    "    for address, city in zip(dataframe.address, dataframe.city):\n",
    "        time.sleep(int(random.randint(0,6)/3)) # try not to get your ip blacklisted by Nominatim\n",
    "        clear_output(wait=True)\n",
    "        location = None\n",
    "        for i in range(NUM_RETRIES):\n",
    "            try:\n",
    "                location = geocoder.geocode(f'{address} {city}')\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        if location:\n",
    "            latlngs.append((location.latitude, location.longitude))\n",
    "        else:\n",
    "            latlngs.append(None)\n",
    "        print(f'{len(latlngs)+1}/{len(dataframe)} complete...')\n",
    "    dataframe['latlng'] = latlngs\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function `generate_latlng()` takes a while to run since we call a web service `Nominatim` to perform the address --> latlng conversion.  \n",
    "Let's use Python's `multiprocessing` module to run these conversions in a background job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively uncomment here to load pre-processed data\n",
    "# df = pd.read_csv('./data/sf/data_w_latlng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "pool = Pool()\n",
    "job = pool.apply_async(func=generate_latlng, args=(df,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether the job is complete using `ready()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if job.ready():\n",
    "    df = job.get()\n",
    "    df.to_csv('./data/sf/data_w_latlng.csv')\n",
    "else:\n",
    "    print(\"job not complete yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `folium` to render the slippy map in the notebook.  \n",
    "Note that there are hundreds of houses to be displayed and this requires a fair bit of RAM. If your browser crashes you can adjust the amount to be displayed by changing the variable `display_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_houses_on_map(dataframe: pd.DataFrame):\n",
    "    dataframe = dataframe.copy()\n",
    "    # create a folium map object centered in SF\n",
    "    m = folium.Map(location=(37.7, -122.4))\n",
    "    # create a colormap of the prices (we limit prices between 5e5 and 10e6)\n",
    "    colors = ['gray', 'green','blue','red','orange', 'yellow']\n",
    "    min_price, max_price = 5e5, 6e6\n",
    "    colormap = StepColormap(colors=colors,vmin=min_price, vmax=max_price, caption='price')\n",
    "    m.add_child(colormap)\n",
    "    # amount of points to render on the map. WARNING: significant RAM required to plot all points and may crash your browser \n",
    "    display_max = len(dataframe) # plot all\n",
    "    # display_max = 100 # uncomment and adjust this number if needed\n",
    "    displayed = 0\n",
    "    for i, latlng in zip(dataframe.index, dataframe['latlng']):\n",
    "        price = dataframe.loc[i, 'price']\n",
    "        if latlng is not None:\n",
    "            if isinstance(latlng, str):\n",
    "                lat, lng = latlng.replace('(','').replace(')','').split(',')\n",
    "                latlng = (float(lat), float(lng))\n",
    "            if not isinstance(latlng, tuple):\n",
    "                continue\n",
    "            style = {'fillColor': colormap(price),\n",
    "                    'color' : colormap(price)}\n",
    "            p = geojson.Point(coordinates=(latlng[1], latlng[0]), style=style)\n",
    "            # build an HTML string to be displayed if we click a marker.\n",
    "            html_info = '<li>Price: ${}</li><li>Property Type: {}</li>'.format(dataframe.loc[i, 'price'], dataframe.loc[i, 'property_type'])\n",
    "            m.add_child(folium.Marker(location=latlng, icon=folium.Icon(color='black', icon_color=colormap(price)), popup=folium.Popup(html=html_info)))\n",
    "            displayed += 1\n",
    "            if displayed > display_max:\n",
    "                break\n",
    "    return m\n",
    "draw_houses_on_map(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe some patterns w.r.t. location.  \n",
    "Seems the more expensive homes are Central and North and the \"lower\" (finger quotes) priced homes on the outside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, let's see how some of the variables interact with the list price.  \n",
    "Since `price` is our target variable (the variable we are trying to predict), it is useful visualize how each variable relates to `price`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sqft\n",
    "Total square footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sqft/saleprice\n",
    "var = 'sqft'\n",
    "sns.regplot(df[var], df['price'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship looks linear with some spreading as sqft increases. We can also see there are some houses with almost zero square feet! Let's investigate why:  \n",
    "  \n",
    "Note on `pandas.DataFrame` indexing:  \n",
    "* `df['sqft'] < condition` gives us a \"truth array\" where True values match the condition and False otherwise. If we index the original DataFrame with this truth array we get a filtered result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the DataFrame with nearly zero sqft\n",
    "df[df['sqft'] < 10].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some bad data from the web scraping. We will remember to remove these when we get to our data cleaning notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bed\n",
    "Number of bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var = 'bed'\n",
    "sns.regplot(df[var], df['price'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a bit of a positive correlation between price and number of beds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bath\n",
    "Number of bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'bath'\n",
    "sns.regplot(df[var], df['price'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive correlation between number of baths and price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix will graphically show us which variables are most correlated to our target variable `price`.  \n",
    "A positive correlation (w.r.t. price) means as the variable increases, the price increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "sns.heatmap(corrmat, vmax=1, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the `price` across a row or column to get an idea which variables are most likely to correlate to price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables.  \n",
    "So far we have only dealt with numeric variables however there are several non-numerical (**Categorical**) variables to be investigated as well:  \n",
    "  \n",
    "Categorical variables are ones which provide information but are not quantified numerically. For instance, the `postal_code` variable gives us information about which neighbourhood the house is located. We found from our map plot that this data may be useful for predicting `price`.  \n",
    "  \n",
    "In order to use these categorical variables in our model, we encode them into a numerical representation called a [Dummy Variable]. We cover Dummy Variables in a later notebook.\n",
    "[Dummy Variable]: https://en.wikipedia.org/wiki/Dummy_variable_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose `property_type`, and `postal_code` to investigate.  \n",
    "We can use the `unique()` function on the categorical columns to see the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df['postal_code'].value_counts())\n",
    "print(df['property_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some variables with only a single value, let's get rid of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postal_codes = [\n",
    "'94530',\n",
    "'94014',\n",
    "'94608',\n",
    "'94607',\n",
    "'94005',\n",
    "'94706',\n",
    "'94501'\n",
    "]\n",
    "for postal_code in postal_codes:\n",
    "    df = df[df.postal_code != int(postal_code)]\n",
    "property_types = ['townhouse', 'foreclosure']\n",
    "for property_type in property_types:\n",
    "    df = df[df.property_type != property_type]\n",
    "print(df['postal_code'].value_counts())\n",
    "print(df['property_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize these categories as box plots.  \n",
    "We use the `pandas.melt()` function to flatten our variables into a single column so we can plot.  \n",
    "The result of using `melt()` is most easily understood by displaying the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vars_to_analyze = ['property_type', 'postal_code']\n",
    "df_melt = pd.melt(df, id_vars=['price'], value_vars=vars_to_analyze)\n",
    "for var in vars_to_analyze:\n",
    "    df_var = df_melt[df_melt['variable'] == var]\n",
    "    sns.boxplot(x=df_var['value'], y=df_var['price'])\n",
    "    x=plt.xticks(rotation=45)\n",
    "    plt.title(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to see the effects of melt()\n",
    "# df_melt.head(20)\n",
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of variance (ANOVA)\n",
    "We use ANOVA to explore how much variance occurs **between** groups (ie. *[price vs property_type]* vs *[price vs postal_code]*) versus how much variance occurs **within** each group (ie *[price vs sub_area]* alone).  \n",
    "In the end this tells us is how useful it will be to group `price` into these 4 groups (and if including each variable in our model is useful to us).  \n",
    "Here's a quick YouTube video that may better explain ANOVA:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(id='ITf4vHhyGpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def anova(df):\n",
    "    anv = pd.DataFrame()\n",
    "    anv['feature'] = vars_to_analyze\n",
    "    pvals = []\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for c in vars_to_analyze:\n",
    "        samples = []\n",
    "        for cls in df[c].unique():\n",
    "            s = df[df[c] == cls]['price'].values\n",
    "            samples.append(s)\n",
    "        try:\n",
    "            pval = stats.f_oneway(*samples)[1]\n",
    "        except Exception as e:\n",
    "            pval=None\n",
    "            print(e)\n",
    "        finally:\n",
    "            pvals.append(pval)\n",
    "    anv['pval'] = pvals\n",
    "    return anv.sort_values('pval')\n",
    "\n",
    "a = anova(df.dropna())\n",
    "a['disparity'] = np.log(1./a['pval'].values)\n",
    "sns.barplot(data=a, x='feature', y='disparity')\n",
    "x=plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a rough estimate of effect each variable will have on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save our DataFrame to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/sf/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopefully the EDA has improved our intuition about the dataset. Now we can move onto data cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
