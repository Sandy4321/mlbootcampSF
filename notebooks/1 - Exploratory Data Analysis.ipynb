{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with an Exploratory Data Analysis (EDA) of the Vancouver housing dataset.  \n",
    "It is always a good idea to start with an EDA before designing and training a machine learning algorithm.  \n",
    "EDA gives us better insight to the data by using statistical and visualization techniques.  \n",
    "\n",
    "Upon completing this notebook, we should have:  \n",
    "* Familiarity with [Pandas] and [NumPy] for data management and analysis\n",
    "* Familiarity with [Matplotlib] and [seaborn] for visualization\n",
    "* A decent understanding of the characteristics of our dataset\n",
    "[Pandas]: https://pandas.pydata.org/\n",
    "[NumPy]: http://www.numpy.org/\n",
    "[Matplotlib]: https://matplotlib.org/\n",
    "[seaborn]: https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy import Nominatim\n",
    "import geojson\n",
    "import folium\n",
    "from branca.colormap import LinearColormap, StepColormap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by loading the data and have a peek at the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/rew_van_jan12.csv') # load contents of .csv into a pandas.DataFrame object\n",
    "df.head(5) # display first 5 entries of DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data is now contained in a variable named `df` which is a pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some quick stats about the DataFrame\n",
    "DataFrame has a few built in functions we can call to get a quick summary of the data:  \n",
    "* `info()` displays a count of all non-null objects and their datatypes  \n",
    "* `describe()` calculates basic statistics about all numerical values in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, very large maximum price albeit not suprising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe only the 'price' column\n",
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have the gist of the dataset size and its contents, it's time to go more in depth and Visualize the data.  \n",
    "We will use `Seaborn` to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histogram of prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globally set our seaborn plot size to 12 by 8 inches:\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "def plot_prices(df: pd.DataFrame, bins: list):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(bins)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    return sns.distplot(df.price, bins=bins)\n",
    "\n",
    "bins = range(int(df.price.min()),int(df.price.max()),1000000)\n",
    "plot_prices(df, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitely a skewed distribution, looks as if we have a few outliers at the higher range of the prices.  \n",
    "### We can quantify this by calculating:  \n",
    "* `Skewness` - A measure of the symmetry (or lack thereof) of a distribution\n",
    "* `Kurtosis` - Whether distrubition is \"heavy-tailed\" or \"light-tailed\" or in other words: how \"sharp\" the peak is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['price'].skew())\n",
    "print(\"Kurtosis: %f\" % df['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot with outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = df[df.price < 15e6]\n",
    "bins = range(int(df_no_outliers.price.min()),int(df_no_outliers.price.max()),500000)\n",
    "plot_prices(df_no_outliers, bins)\n",
    "print(\"Skewness (outliers removed): %f\" % df_no_outliers['price'].skew())\n",
    "print(\"Kurtosis (outliers removed): %f\" % df_no_outliers['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the outliers improved our skewness and kurtosis values.\n",
    "We will remember this when cleaning the data for our model. Machine learning models work best with normally distributed data. Outliers may affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot missing values.\n",
    "Recall that there were some columns which are incomplete. Plot a bar graph describing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that are missing values can either be removed from the dataset or have their missing values replaced (perhaps with 0 or the mean of the column). Remember this for data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since we have geolocations of the houses in the `latlng` column, let's visualize the data on a slippy map and see if there are any patterns w.r.t. price.  \n",
    "We use `folium` to render html in the notebook.  \n",
    "Note that there are hundreds of houses to be displayed and this requires a lot of RAM. If your browser crashes you can adjust the amount to be displayed by changing the variable `display_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a folium map object centered in Vancouver\n",
    "m = folium.Map(location=(49.271554, -123.106738))\n",
    "# create a colormap of the prices (we limit prices between 5e5 and 10e6)\n",
    "colors = ['gray', 'green','blue','red','orange', 'yellow']\n",
    "min_price, max_price = 5e5, 10e6\n",
    "colormap = StepColormap(colors=colors,vmin=min_price, vmax=max_price, caption='price')\n",
    "m.add_child(colormap)\n",
    "# amount of points to render on the map. WARNING: significant RAM required to plot all points and may crash your browser \n",
    "display_max = len(df) # plot all\n",
    "# display_max = 100 # uncomment and adjust this number if needed\n",
    "displayed = 0\n",
    "for i, latlng in enumerate(df['latlng']):\n",
    "    price = df.loc[i, 'price']\n",
    "    if latlng is not None and latlng != 'MISSING':\n",
    "        if isinstance(latlng, str):\n",
    "            lat, lng = latlng.replace('(','').replace(')','').split(',')\n",
    "            latlng = (float(lat), float(lng))\n",
    "        if not isinstance(latlng, tuple):\n",
    "            continue\n",
    "        style = {'fillColor': colormap(price),\n",
    "                'color' : colormap(price)}\n",
    "        p = geojson.Point(coordinates=(latlng[1], latlng[0]), style=style)\n",
    "        # build an HTML string to be displayed if we click a marker.\n",
    "        html_info = '<li>Price: ${}</li><li>Property Type: {}</li>'.format(df.loc[i, 'price'], df.loc[i, 'property_type'])\n",
    "        m.add_child(folium.Marker(location=latlng, icon=folium.Icon(color='black', icon_color=colormap(price)), popup=folium.Popup(html=html_info)))\n",
    "        displayed += 1\n",
    "        if displayed > display_max:\n",
    "            break\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can observe some patterns w.r.t. location with higher prices in the West where many exclusive communities are located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, let's see how some of the variables interact with the list price.  \n",
    "Since `price` is our target variable (the variable we are trying to predict), it is useful visualize how each variable relates to `price`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sqft\n",
    "Total square footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sqft/saleprice\n",
    "var = 'sqft'\n",
    "sns.regplot(df[var], df['price'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship looks linear with some spreading as sqft increases. We can also see there are some houses with zero square feet! Let's investigate why:  \n",
    "  \n",
    "Note on `pandas.DataFrame` indexing:  \n",
    "* `df['sqft'] == 0` gives us a \"truth array\" where True values match the condition and False otherwise. If we index the original DataFrame with this truth array we get a filtered result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the DataFrame with zero sqft\n",
    "df[df['sqft'] == 0].head(3) # only display first 3 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `property_type` of these are *Land/Lot*. We will remember to remove these when we get to our data cleaning notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bed\n",
    "Number of bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var = 'bed'\n",
    "sns.regplot(df[var], df['price'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship is non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bath\n",
    "Number of bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'bath'\n",
    "sns.regplot(df[var], df['price'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix will graphically show us which variables are most correlated to our target variable `price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "sns.heatmap(corrmat, vmax=1, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm something funny is going on with the `age` variable. Absolutely zero correlation is a hint there may be corrupt values.  \n",
    "Let's investigate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['age'] < 0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough there are some strange negative values in age. Remember this for data cleaning.  \n",
    "Plot our correlation matrix again omitting negative `age` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_neg_age = df[df['age'] >= 0]\n",
    "corrmat = df_no_neg_age.corr()\n",
    "sns.heatmap(corrmat, vmax=1, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables.  \n",
    "So far we have only dealt with numeric variables however there are several non-numerical (**Categorical**) variables to be investigated as well.  \n",
    "Categorical variables are ones which provide information but are not quantified numerically. For instance, the `sub_area` variable gives us information about what neighbourhood the house is located in (ie. \"Kerrisdale\", \"Yaletown\" etc.). From our map plot, we found this information is important when considering house prices.  \n",
    "In order to use these categorical variables in our model, we encode them into a numerical representation called a [Dummy Variable]. We cover Dummy Variables in a later notebook.\n",
    "[Dummy Variable]: https://en.wikipedia.org/wiki/Dummy_variable_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose `area`, `sub_area`, `property_type`, and `strata_type` to investigate.  \n",
    "We can use the `unique()` function on the categorical columns to see the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df['area'].unique())\n",
    "print(df['sub_area'].unique())\n",
    "print(df['property_type'].unique())\n",
    "print(df['strata_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are **2** `area`, **39** `sub_area`, **7** `property_type` and **8** `strata_type` categories.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize these 4 categories as box plots.  \n",
    "We use the `pandas.melt()` function to flatten our variables into a single column so we can plot.  \n",
    "The result of using `melt()` is most easily understood by displaying the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vars_to_analyze = ['area', 'sub_area', 'property_type', 'strata_type']\n",
    "df_melt = pd.melt(df, id_vars=['price'], value_vars=vars_to_analyze)\n",
    "for var in vars_to_analyze:\n",
    "    df_var = df_melt[df_melt['variable'] == var]\n",
    "    sns.boxplot(x=df_var['value'], y=df_var['price'])\n",
    "    x=plt.xticks(rotation=90)\n",
    "    plt.title(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of variance (ANOVA)\n",
    "We use ANOVA to explore how much variance occurs **between** groups (ie. *[price vs sub_area]* vs *[price vs area]* vs *[price vs property_type]* vs *[price vs strata_type]*) versus how much variance occurs **within** each group (ie *price vs sub_area* alone).  \n",
    "In the end this tells us is how useful it will be to group `price` into these 4 groups (and if including each variable in our model is useful to us).  \n",
    "Here's a quick YouTube video that may better explain ANOVA:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(id='ITf4vHhyGpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def anova(df):\n",
    "    anv = pd.DataFrame()\n",
    "    anv['feature'] = vars_to_analyze\n",
    "    pvals = []\n",
    "    for c in vars_to_analyze:\n",
    "        samples = []\n",
    "        for cls in df[c].unique():\n",
    "            s = df[df[c] == cls]['price'].values\n",
    "            samples.append(s)\n",
    "        try:\n",
    "            pval = stats.f_oneway(*samples)[1]\n",
    "        except Exception as e:\n",
    "            pval=None\n",
    "        finally:\n",
    "            pvals.append(pval)\n",
    "    anv['pval'] = pvals\n",
    "    return anv.sort_values('pval')\n",
    "\n",
    "a = anova(df)\n",
    "a['disparity'] = np.log(1./a['pval'].values)\n",
    "sns.barplot(data=a, x='feature', y='disparity')\n",
    "x=plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a rough estimate of effect each variable will have on our model. It makes intuitive sense that `sub_area` is highest since a home in Point Grey is likely to be more expensive than one in Grandview. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopefully the EDA has improved our intuition about the dataset. Now we can move onto data cleaning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
